{
  "content": "\"\"\" This module extends standard Python's pickle module so that it is\ncapable of writing more efficient pickle files that contain Panda\nobjects with shared pointers.  In particular, a single Python\nstructure that contains many NodePaths into the same scene graph will\nwrite the NodePaths correctly when used with this pickle module, so\nthat when it is unpickled later, the NodePaths will still reference\ninto the same scene graph together.\n\nIf you use the standard pickle module instead, the NodePaths will each\nduplicate its own copy of its scene graph.\n\nThis is necessary because the standard pickle module doesn't provide a\nmechanism for sharing context between different objects written to the\nsame pickle stream, so each NodePath has to write itself without\nknowing about the other NodePaths that will also be writing to the\nsame stream.  This replacement module solves this problem by defining\na ``__reduce_persist__()`` replacement method for ``__reduce__()``,\nwhich accepts a pointer to the Pickler object itself, allowing for\nshared context between all objects written by that Pickler.\n\nUnfortunately, cPickle cannot be supported, because it does not\nsupport extensions of this nature. \"\"\"\n\n__all__ = [\"PickleError\", \"PicklingError\", \"UnpicklingError\", \"Pickler\",\n           \"Unpickler\", \"dump\", \"dumps\", \"load\", \"loads\",\n           \"HIGHEST_PROTOCOL\", \"DEFAULT_PROTOCOL\"]\n\nimport sys\nfrom panda3d.core import BamWriter, BamReader, TypedObject\nfrom copyreg import dispatch_table\n\n\n# A funny replacement for \"import pickle\" so we don't get confused\n# with the local pickle.py.\npickle = __import__('pickle')\n\nHIGHEST_PROTOCOL = pickle.HIGHEST_PROTOCOL\nDEFAULT_PROTOCOL = pickle.DEFAULT_PROTOCOL\n\nPickleError = pickle.PickleError\nPicklingError = pickle.PicklingError\nUnpicklingError = pickle.UnpicklingError\n\nBasePickler = pickle._Pickler\nBaseUnpickler = pickle._Unpickler\n\n\nclass Pickler(BasePickler):  # type: ignore[misc, valid-type]\n\n    def __init__(self, *args, **kw):\n        self.bamWriter = BamWriter()\n        self._canonical = {}\n        BasePickler.__init__(self, *args, **kw)\n\n    def clear_memo(self):\n        BasePickler.clear_memo(self)\n        self._canonical.clear()\n        self.bamWriter = BamWriter()\n\n    # We have to duplicate most of the save() method, so we can add\n    # support for __reduce_persist__().\n\n    def save(self, obj, save_persistent_id=True):\n        if self.proto >= 4:\n            self.framer.commit_frame()\n\n        # Check for persistent id (defined by a subclass)\n        pid = self.persistent_id(obj)\n        if pid is not None and save_persistent_id:\n            self.save_pers(pid)\n            return\n\n        # Check if this is a Panda type that we've already saved; if so, store\n        # a mapping to the canonical copy, so that Python's memoization system\n        # works properly.  This is needed because Python uses id(obj) for\n        # memoization, but there may be multiple Python wrappers for the same\n        # C++ pointer, and we don't want that to result in duplication.\n        t = type(obj)\n        if issubclass(t, TypedObject.__base__):\n            canonical = self._canonical.get(obj.this)\n            if canonical is not None:\n                obj = canonical\n            else:\n                # First time we're seeing this C++ pointer; save it as the\n                # \"canonical\" version.\n                self._canonical[obj.this] = obj\n\n        # Check the memo\n        x = self.memo.get(id(obj))\n        if x:\n            self.write(self.get(x[0]))\n            return\n\n        # Check the type dispatch table\n        f = self.dispatch.get(t)\n        if f:\n            f(self, obj) # Call unbound method with explicit self\n            return\n\n        # Check for a class with a custom metaclass; treat as regular class\n        try:\n            issc = issubclass(t, type)\n        except TypeError: # t is not a class (old Boost; see SF #502085)\n            issc = 0\n        if issc:\n            self.save_global(obj)\n            return\n\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(t)\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # New code: check for a __reduce_persist__ method, then\n            # fall back to standard methods.\n            reduce = getattr(obj, \"__reduce_persist__\", None)\n            if reduce:\n                rv = reduce(self)\n            else:\n                # Check for a __reduce_ex__ method, fall back to __reduce__\n                reduce = getattr(obj, \"__reduce_ex__\", None)\n                if reduce:\n                    rv = reduce(self.proto)\n                else:\n                    reduce = getattr(obj, \"__reduce__\", None)\n                    if reduce:\n                        rv = reduce()\n                    else:\n                        raise PicklingError(\"Can't pickle %r object: %r\" %\n                                            (t.__name__, obj))\n\n        # Check for string returned by reduce(), meaning \"save as global\"\n        if type(rv) is str:\n            self.save_global(obj, rv)\n            return\n\n        # Assert that reduce() returned a tuple\n        if type(rv) is not tuple:\n            raise PicklingError(\"%s must return string or tuple\" % reduce)\n\n        # Assert that it returned an appropriately sized tuple\n        l = len(rv)\n        if not (2 <= l <= 5):\n            raise PicklingError(\"Tuple returned by %s must have \"\n                                \"two to five elements\" % reduce)\n\n        # Save the reduce() output and finally memoize the object\n        self.save_reduce(obj=obj, *rv)\n\n\nclass Unpickler(BaseUnpickler):  # type: ignore[misc, valid-type]\n\n    def __init__(self, *args, **kw):\n        self.bamReader = BamReader()\n        BaseUnpickler.__init__(self, *args, **kw)\n\n    # Duplicate the load_reduce() function, to provide a special case\n    # for the reduction function.\n\n    def load_reduce(self):\n        stack = self.stack\n        args = stack.pop()\n        func = stack[-1]\n\n        # If the function name ends with \"_persist\", then assume the\n        # function wants the Unpickler as the first parameter.\n        func_name = func.__name__\n        if func_name.endswith('_persist') or func_name.endswith('Persist'):\n            value = func(self, *args)\n        else:\n            # Otherwise, use the existing pickle convention.\n            value = func(*args)\n\n        stack[-1] = value\n\n    BaseUnpickler.dispatch[pickle.REDUCE[0]] = load_reduce\n\n\n# Shorthands\nfrom io import BytesIO\n\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\ndef dumps(obj, protocol=None):\n    file = BytesIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\ndef load(file):\n    return Unpickler(file).load()\n\ndef loads(str):\n    file = BytesIO(str)\n    return Unpickler(file).load()\n",
  "metadata": {
    "source": "corpus_panda3d/source_code/direct/src/stdpy/pickle.py",
    "doc_type": "python",
    "file_path": "processed_documents/context_documents/90f27aca_pickle.json",
    "doc_id": "doc_207"
  }
}